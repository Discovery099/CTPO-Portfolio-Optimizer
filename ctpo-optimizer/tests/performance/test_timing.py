"""\nPerformance and timing tests\n"""\n\nimport pytest\nimport numpy as np\nimport time\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent.parent))\n\nfrom ctpo.core.optimizer import CTPOOptimizer\n\n@pytest.fixture\ndef timing_data():\n    \"\"\"Generate data for timing tests\"\"\"\n    np.random.seed(42)\n    sizes = [10, 20, 50]\n    data = {}\n    \n    for n in sizes:\n        returns = np.random.normal(0.0005, 0.02, (252, n))\n        data[n] = returns\n    \n    return data\n\n@pytest.mark.performance\nclass TestPerformanceTiming:\n    \"\"\"Performance benchmarks\"\"\"\n    \n    def test_solve_time_10_assets(self, timing_data):\n        \"\"\"Test solve time with 10 assets\"\"\"\n        optimizer = CTPOOptimizer()\n        returns = timing_data[10]\n        \n        start = time.perf_counter()\n        weights = optimizer.optimize(returns)\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        \n        print(f\"\\n‚è±Ô∏è  10 assets: {elapsed_ms:.2f} ms\")\n        assert elapsed_ms < 50, \\\n            f\"Solve time {elapsed_ms:.1f}ms exceeds 50ms target\"\n    \n    def test_solve_time_20_assets(self, timing_data):\n        \"\"\"Test solve time with 20 assets\"\"\"\n        optimizer = CTPOOptimizer()\n        returns = timing_data[20]\n        \n        start = time.perf_counter()\n        weights = optimizer.optimize(returns)\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        \n        print(f\"‚è±Ô∏è  20 assets: {elapsed_ms:.2f} ms\")\n        assert elapsed_ms < 100, \\\n            f\"Solve time {elapsed_ms:.1f}ms exceeds 100ms\"\n    \n    def test_solve_time_50_assets(self, timing_data):\n        \"\"\"Test solve time with 50 assets\"\"\"\n        optimizer = CTPOOptimizer()\n        returns = timing_data[50]\n        \n        start = time.perf_counter()\n        weights = optimizer.optimize(returns)\n        elapsed_ms = (time.perf_counter() - start) * 1000\n        \n        print(f\"‚è±Ô∏è  50 assets: {elapsed_ms:.2f} ms\")\n        assert elapsed_ms < 200, \\\n            f\"Solve time {elapsed_ms:.1f}ms exceeds 200ms\"\n    \n    def test_repeated_optimization_speed(self, timing_data):\n        \"\"\"Test speed of repeated optimizations\"\"\"\n        optimizer = CTPOOptimizer()\n        returns = timing_data[10]\n        \n        times = []\n        n_runs = 10\n        \n        for _ in range(n_runs):\n            start = time.perf_counter()\n            weights = optimizer.optimize(returns)\n            elapsed = time.perf_counter() - start\n            times.append(elapsed)\n        \n        avg_time_ms = np.mean(times) * 1000\n        std_time_ms = np.std(times) * 1000\n        \n        print(f\"\\nüìä Repeated runs ({n_runs}x):\")\n        print(f\"   Average: {avg_time_ms:.2f} ms\")\n        print(f\"   Std Dev: {std_time_ms:.2f} ms\")\n        \n        assert avg_time_ms < 50\n        assert std_time_ms < 20, \"Timing should be consistent\"\n\n\nif __name__ == '__main__':\n    pytest.main([__file__, '-v', '-s'])\n